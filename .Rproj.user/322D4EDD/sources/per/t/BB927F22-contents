
# This script contains all data preparation steps, modelling and graphs -
  # only the most important parts and graphs are shown, while other chunks fo the code are hidden for readability

# SET-UP

  # PACKAGES
  {
    requiredPackages <- c('rjson', 'geojsonio', 'osmdata', 'tidyverse', 'sf', 'sp', 'hddtools', 'rgeos', 'raster', 'lwgeom', 'units', 'gtools', 'reshape2', 
                  'openxlsx', 'tigris', 'broom', 'viridis', 'gridExtra', 'corrplot', 'psych', 'cowplot', 'randomForest', 'Metrics', 'e1071', 'caret',
                  'xgboost', 'Matrix', 'OneR', 'mgcv', 'visreg', 'ALEPlot', 'vip', 'pdp', 'glmnet', 'MASS', 'lm.beta', 'viridis', 'gradDescent', 'plyr')
    
    for (i in requiredPackages) {
      if (!require(i,character.only = T)) {
        install.packages(i)
      }
    }
  }
  
  # DATA
  {
    rm(list=ls())
    
    options(scipen=999)
    
    setwd('/Users/Szymon/Desktop/UNI/Mag/3 semester/Adv Viz/project/R')
    
    panel <- read.xlsx('data/xlsx/suburbanisation.xlsx')
  }
  

# DATA PREPARATION

  # STEP 1 - BOROUGHS OSM RELATION IDs
  # source: OSM site
  # ! relatively long computation time !
  {
    # COUNTIES
    links <- (unique(paste('gmina%20', panel$borough, '%2C%20powiat%20', panel$county, sep='')))
    boroughs_osm <- data.frame(borough=character(), county=character(), osm_id=integer(), stringsAsFactors = F)
    
    for (i in (1:length(links))) {
      boroughs_osm[i,'borough'] <- substr(links[i],9,gregexpr('%20', links[i])[[1]][2]-4)
      boroughs_osm[i,'county'] <- substr(links[i],gregexpr('%20', links[i])[[1]][3]+3,100)
      download.file(url=paste('https://nominatim.openstreetmap.org/search?q=', links[i],'&format=json', sep=''),
                    destfile = 'data/json/osm_id.json')
      osm_id <- fromJSON(file = 'data/json/osm_id.json')
      if (length(osm_id)==0) {
        boroughs_osm[i,'osm_id'] <- NA
      }
      else {
        boroughs_osm[i,'osm_id'] <- osm_id[[1]]$osm_id
      }
      Sys.sleep(runif(1,1,3))
    }
    
    # CITIES
    boroughs_osm$borough <- as.factor(boroughs_osm$borough)
    boroughs_osm$county <- as.factor(boroughs_osm$county)
    links_na <- (unique(paste(boroughs_osm[is.na(boroughs_osm$osm_id)==T,'borough'], '%2C%20powiat%20', boroughs_osm[is.na(boroughs_osm$osm_id)==T,'county'], sep='')))
    
    for (i in (1:length(links_na))) {
      download.file(url=paste('https://nominatim.openstreetmap.org/search?q=', links_na[i],'&format=json', sep=''),
                    destfile = 'data/json/osm_id.json')
      osm_id <- fromJSON(file = 'data/json/osm_id.json')
      if (length(osm_id)==0) {
        boroughs_osm[boroughs_osm$borough==substr(links_na[i],1,gregexpr('%20', links_na[i])[[1]][2]-4),'osm_id'] <- NA
      }
      else {
        boroughs_osm[boroughs_osm$borough==substr(links_na[i],1,gregexpr('%20', links_na[i])[[1]][1]-4),'osm_id'] <- osm_id[[1]]$osm_id
      }
      Sys.sleep(runif(1,1,2))
    }
    
    # NAs (Podkowa Leśna -> powiat grodziski, not pruszkowski?)
    download.file(url=paste('https://nominatim.openstreetmap.org/search?q=Podkowa Leśna%2C%20powiat%20grodziski&format=json', sep=''),
                  destfile = 'data/json/osm_id.json')
    osm_id <- fromJSON(file = 'data/json/osm_id.json')
    boroughs_osm[boroughs_osm$borough=='Podkowa Leśna','osm_id'] <- osm_id[[1]]$osm_id
    
    # save results
    write.csv2(boroughs_osm, file='data/csv/boroughs_osm.csv')
  }
  
  # STEP 2 -DOWNLOAD LOCATION POLYGONS AND BUILDINGS AND ROADS WITHIN
  # source: OSM pacakge
  # ! long computation time !
  {
    # load saved file
    boroughs_osm <- read.csv('data/csv/boroughs_osm.csv', sep=';')
    
      # define function to trim osm data when there are multiple bounding polygons (e.g. borough Brwinów)
    trim_osmdata_mltpl <- function(data, bbox, exclude=T) {
        # extract polygons from Spatial DataFrame
      polygons <- bbox@polygons[[1]]@Polygons
    
        # check if there are multiple and process accordingly
      for (pol in loc_sp@polygons[[1]]@Polygons) {
        res_tmp <- trim_osmdata(data, pol@coords, exclude)
        if (! exists('res')) {
          res <- res_tmp
        } else {
          res <- c(res, res_tmp)
        }
      }
      return(res)
    }
    
      # accomodation building type (also keeping 'yes' as it's a very common value)
    bldgs_tp = c('apartments', 'bungalow', 'cabin', 'detached', 'dormitory', 'farm', 'house', 'residential', 'semidetached_house', 'terrace', 'yes')
      # highway types
    hghws_tp <- c('motorway', 'trunk', 'primary', 'secondary', 'tertiary', 'unclassified', 'residential', 'living_street', 'road')
    
    for (borough in boroughs_osm[, 2]) {
      # remove locs, bldgs, hghws in first iteration and set counter
      if (borough == 'Baranów') {
        try(rm(locs, bldgs, hghws))
        i <- 1
      }
    #
      # extract borough polygon and bounding box
      loc <- opq_osm_id(id=boroughs_osm[boroughs_osm$borough==borough,'osm_id'], type='relation') %>%
        opq_string () %>%
        osmdata_sf()
      loc$osm_multipolygons$borough = borough
      loc_sp <- as(loc$osm_multipolygons, 'Spatial')
      loc_bb <- loc_sp@bbox
      locs <- ifelse(exists('locs'), c(locs, loc), loc)
    
      # extract buildings within the borough
      bldg <- opq(bbox = loc_bb) %>%
        add_osm_feature(key = 'building', value = bldgs_tp) %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., loc_sp, F)
      bldg$osm_polygons$borough = borough
      bldgs <- ifelse(exists('bldgs'), c(bldgs, bldg), bldg)
    
      # extract highways within the borough
      hghw <- opq(bbox = loc_bb) %>%
        add_osm_feature(key = 'highway', value = hghws_tp) %>%
        osmdata_sf() %>%
        trim_osmdata_mltpl(., loc_sp, F)
      hghw$osm_lines$borough = borough
      hghws <- ifelse(exists('hghws'), c(hghws, hghw), hghw)
    
      # progress
      cat(round(100* i/nrow(boroughs_osm), 2), '% done!', '\n')
      i <- i + 1
    
      # remove i and borough and save RDS files after last iteration
      if (i == nrow(boroughs_osm)+1) {
        rm(borough, i)
        saveRDS(locs, 'data/RDS/locs.RDS')
        saveRDS(bldgs, 'data/RDS/bldgs.RDS')
        saveRDS(hghws, 'data/RDS/hghws.RDS')
      }
    }
  }
  
  # STEP 3 -CALCULATE BUILDING-TO-BUILDING DISTANCE & METROPOLITAN DENSITY & BUILDING-TO-HIGHWAY DISTANCE
  # source: own calculations
  # ! very long computation time !
  {
    #columns that would be filled only for non-residential buildings
    bldgs_na_cols <- c('abandoned.building', 'aeroway', 'amenity', 'atm', 'automated', 'barrier', 'beds', 'bench', 'bicycle_parking', 'brand', 'brewery', 'camp_site', 'car_wash', 'castle_type', 'clothes',
                       'community_centre', 'construction', 'cuisine', 'deanery', 'delivery', 'demolished.building', 'denomination', 'diaper', 'diocese', 'disabled.amenity', 'dispensing', 'disused', 'drive_through',
                       'emergency', 'fee', 'government', 'guest_house', 'healthcare', 'heritage', 'highway', 'historic', 'historical', 'horse', 'Kancelaria', 'leisure', 'memorial', 'microbrewery', 'military', 'museum',
                       'natural', 'office', 'public_transport', 'railway', 'rooms', 'ruin', 'ruins', 'second_hand', 'self_service', 'service', 'shelter', 'shelter_type', 'shop', 'shop_1', 'social_facility',
                       'social_facility.for', 'sport', 'street_cabinet', 'takeaway', 'telecom', 'toilets', 'tourism', 'trade', 'wholesale')
      
    metrics <- data.frame(borough = panel$borough)
    
    for (borough in metrics[, 'borough']) {
    
      # set counter
      if (borough==metrics[1, 'borough']) {i <- 1}
    
      # building-to-building distance
      # filter data
      bldgs_borough <- bldgs$osm_polygons %>%
        filter_at(bldgs_na_cols, all_vars(is.na(.))) %>%
        filter(borough == (!! borough))
      # centroids
      bldgs_borough_cent <- as(
        st_centroid(
          st_geometry(
            st_transform(
              bldgs_borough,
              crs = 2180))),
        'Spatial')
      # building-to-building min distance matrix
      bldgs_borough_dist_mat <- gDistance(bldgs_borough_cent, byid = T)
      bldgs_borough_dist_min <- apply(bldgs_borough_dist_mat, 1, function(x) {min(x[x > 0])})
      # metrics
      metrics[metrics$borough==borough, 'bldgs_dist_mean'] <- mean(bldgs_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_dist_median'] <- median(bldgs_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_dist_min'] <- min(bldgs_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_dist_max'] <- max(bldgs_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_dist_sd'] <- sd(bldgs_borough_dist_min)
    
      # Metropolitan Density (MD): DU or RP (...) divided by total land area [LIT]
      MD <- length(bldgs_borough_cent) / set_units(st_area(locs$osm_multipolygons[locs$osm_multipolygons$gmina %in% borough,]), km^2)
      metrics[metrics$borough==borough, 'MD'] <- MD
    
      # building-to-highway distance
      # highways with correct CRS
      hghws_lrg_borough <- as(
        st_transform(
          hghws_lrg[hghws_lrg$gmina %in% borough,],
          crs = 2180),
        'Spatial')
      # house-to-highway min distance matrix
      bldgs_hghws_borough_dist_mat <- gDistance(bldgs_borough_cent, hghws_lrg_borough, byid = T)
      bldgs_hghws_borough_dist_min <- apply(bldgs_hghws_borough_dist_mat, 2, function(x) {min(x[x > 0])})
      # metrics
      metrics[metrics$borough==borough, 'bldgs_hghws_dist_mean'] <- mean(bldgs_hghws_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_hghws_dist_median'] <- median(bldgs_hghws_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_hghws_dist_min'] <- min(bldgs_hghws_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_hghws_dist_max'] <- max(bldgs_hghws_borough_dist_min)
      metrics[metrics$borough==borough, 'bldgs_hghws_dist_sd'] <- sd(bldgs_hghws_borough_dist_min)
    
      # extract borough polygon and bounding box
      borough_sp <- as(locs$osm_multipolygon[locs$osm_multipolygon$gmina %in% borough,], 'Spatial')
      borough_bb <- borough_sp@bbox
    
      # find shops
      shop <- opq(bbox = borough_bb) %>%
        add_osm_feature(key = 'shop') %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., borough_sp, F)
    
      # find toursit sites
      trsm <- opq(bbox = borough_bb) %>%
        add_osm_feature(key = 'tourism') %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., borough_sp, F)
    
      # find leisure sites
      lsr <- opq(bbox = borough_bb) %>%
        add_osm_feature(key = 'leisure') %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., borough_sp, F)
    
      # find sport sites
      sprt <-  opq(bbox = borough_bb) %>%
        add_osm_feature(key = 'sport') %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., borough_sp, F)
    
      # find places of worship
      amnts <-  opq(bbox = borough_bb) %>%
        add_osm_feature(key = 'amenity') %>%
        osmdata_sf()  %>%
        trim_osmdata_mltpl(., borough_sp, F)
    
      # append results to metrics
      metrics[metrics$borough==borough, 'shops_cnt'] <- nrow(shop$osm_polygons) + nrow(shop$osm_points)
      metrics[metrics$borough==borough, 'tourist_sites_cnt'] <- nrow(trsm$osm_polygons) + nrow(trsm$osm_points)
      metrics[metrics$borough==borough, 'leisure_sites_cnt'] <- nrow(lsr$osm_polygons) + nrow(lsr$osm_points)
      metrics[metrics$borough==borough, 'sport_sites_cnt'] <- nrow(sprt$osm_polygons) +  nrow(sprt$osm_points)
      metrics[metrics$borough==borough, 'worship_sites_cnt'] <- nrow(amnts$osm_polygons[na.replace(amnts$osm_polygons$amenity == 'place_of_worship', F),]) + nrow(amnts$osm_points[na.replace(amnts$osm_points$amenity == 'place_of_worship', F),])
      metrics[metrics$borough==borough, 'restaurants_cnt'] <- nrow(amnts$osm_polygons[na.replace(amnts$osm_polygons$amenity %in% rstrnt_types, F),]) + nrow(amnts$osm_points[na.replace(amnts$osm_points$amenity %in% rstrnt_types, F),])
    
      # print progress
      cat(round(100* i/length(locs), 2), '% done! \n')
      i <- i+1
    
        if (i == nrow( metrics[, 'borough'])+1) {
          rm(borough, i)
          write.csv(metrics, 'data/csv/metrics.csv')
        }
    }
  }
  
  # STEP 4 - CALCULATE MIN DRIVING DISTANCE AND TIME FROM THE BOROUGH'S CENTRE TO WARSAW CENTRAL STATION
  # source: Google API & own calculations
  # ! long computation time !
  {
    for (borough in metrics[, 'borough']) {
    
      if (borough==boroughs[1]) {i <- 1}
    
      # select borough's polygon and mao to correct CRS
      loc_sp <- locs$osm_multipolygon[locs$osm_multipolygon$gmina %in% borough,]
      cent <- coordinates(
        as(
          st_transform(
            st_centroid(
              st_geometry(
                st_transform(
                  loc_sp,
                  crs = 2180))),
            crs= "+proj=longlat +datum=WGS84 +units=m +no_defs"),
          'Spatial')
      )
    
      # create URL to send to Google API
      cent <- paste(cent[1,2], '+', cent[1,1], sep='')
      url <- paste('https://maps.googleapis.com/maps/api/distancematrix/json?origins=', cent, '&destinations=52.228914+21.003233&mode=driving&units=metrics&key=AIzaSyDwp9rZrUXbESAJ1Oeub-ntBBsqDNA5xJk', sep='')
    
      # read results
      json <- rjson::fromJSON(file = url)
    
      # add result to df
      if (json$rows[[1]]$elements[[1]]$status == "ZERO_RESULTS") {
        metrics[metrics$borough == borough, 'dist_waw_drive'] <- NA
        metrics[metrics$borough == borough, 'time_waw_drive'] <- NA
      } else {
        metrics[metrics$borough == borough, 'dist_waw_drive'] <- json$rows[[1]]$elements[[1]]$distance$value
        metrics[metrics$borough == borough, 'time_waw_drive'] <- json$rows[[1]]$elements[[1]]$duration$value
      }
    
      if (i == nrow(metrics[, 'borough'])+1) {
        rm(borough, i)
        metrics$dist_waw_drive <- metrics$dist_waw_drive/1000
        write.csv(metrics, 'data/csv/metrics.csv', row.names = F)
      }
    }
  }
  
  # STEP 5 - ADD MIN TIMES OF TRANSIT FROM THE BOROUGH'S CENTRE TO WARSAW CENTRAL STATION AND MERGE ALL RESULTS
  # source : e-podróżnik & local sites
  {
    # load data from previous steps
    metrics <- read.csv('data/csv/metrics.csv', sep=';')
    
    # load data
    min_times <- read.xlsx('data/xlsx/boroughs_mt.xlsx')
    
    # merge dataframes
    panel_metrics <- merge(merge(panel, metrics, by='borough'), min_times[, c(1,3)], by='borough')
    panel_metrics$time_waw_drive <- round(panel_metrics$time_waw_drive/60, 2)
  }
  
  # STEP 6 - ADD PARCEL PRICES
  # source: gratka.pl
  {
    # load data
    gratka <- read.csv('data/csv/offers.csv', sep=',')
    
    # filter the data and create numeric columns
    gratka <- gratka %>%
      filter(
        price.PLN. != 'Zapytajocenę',
        price.PLN. != '',
        size.m2. != ''
      ) %>%
      mutate(
        borough = as.factor(borough),
        price = as.numeric(str_replace(price.PLN., ',', '.')),
        size =  as.numeric(str_replace(size.m2., ',', '.')),
        price.m2 = price / size,
        type_eng = plyr::mapvalues(type, c('dom', 'mieszkanie', 'działka'), c('estate', 'estate', 'parcel'))
      )
    
    # calcualte avg and median per type and select desired columns
    gratka_prices <- gratka %>%
      group_by(borough, type_eng) %>%
      summarise(
        n = n(),
        avg = mean(price.m2, na.rm = T),
        med = median(price.m2, na.rm = T)
      ) %>%
      ungroup() %>%
      melt() %>%
      dcast(., borough ~ type_eng + variable, value.var = 'value') %>%
      as.data.frame() %>%
      dplyr::select(borough, parcel_n, parcel_avg, parcel_med)
    
      # create 2 observations for Miński Mazowiecki 1 and 2 (both with same values)
    gratka_prices <- rbind(gratka_prices, gratka_prices[gratka_prices$borough=='Mińsk Mazowiecki',])
    gratka_prices$borough <- as.character(gratka_prices$borough)
    gratka_prices[35, 'borough'] <- 'Mińsk Mazowiecki 1'
    gratka_prices[gratka_prices$borough=='Mińsk Mazowiecki', 'borough'] <- 'Mińsk Mazowiecki 2'
    gratka_prices$borough <- as.factor(gratka_prices$borough)
    
      # merge dataframes
    panel_metrics <- merge(panel_metrics, gratka_prices, by='borough')
    panel_metrics[, c('X', 'dist_waw_transit', 'time_waw_transit')] <- NULL
    
    panel_metrics <- panel_metrics[, c(2,1,3:45)]
      
    # save results
    write.xlsx(panel_metrics, 'data/xlsx/panel_metrics.xlsx', fileEncoding = "UTF-8")
  }


# MODELS

  # STEP 1 - INITIAL SET-UP
  {
      # LOAD DATA
      
      panel_metrics <- read.xlsx('data/xlsx/panel_metrics.xlsx')
      data_model <- panel_metrics[, c(4, 6, 10:17, 19:20, 22:23, 28, 34:36, 40, 42:44)]

      # CUSTOM FUNCTIONS
    
      r2 <- function (actual, preds) {
        rss <- sum((preds - actual) ^ 2)
        tss <- sum((actual - mean(actual)) ^ 2)
        r2 <- 1 - rss/tss
        return (r2)
      }
      
      r2_adj <- function (actual, preds, p = 29) {
        n <- length(actual)
        r2_adj <- 1 - ((1 - r2(actual, preds)) * (n - 1) / (n - p - 1))
        return (r2_adj)
      }
      
      # STATS DF
      
      model_stats <- data.frame(
        "model" = character(),
        "rmse_valid" = numeric(),
        #  "rmsle_valid" = numeric(),
        "mae_valid" = numeric(),
        "r2_valid" = numeric(),
        "rmse_train" = numeric(),
        #  "rmsle_valid" = numeric(),
        "mae_train" = numeric(),
        "r2_train" = numeric()  
      )
    }

  # STEP 2 - RIDGE REGRESSION
  {
    y <- as.matrix(data_model[,1])
    x <- as.matrix(data_model[,-1])
    
    # choosing optimal lambda; nfold=70 = LOOCV
    set.seed(435632)
    cv.out=cv.glmnet(x,y, nfold=70, alpha=0, grouped = FALSE)
    plot(cv.out)
    ridge_bestlam <- cv.out$lambda.min
    ridge_bestlam
    
    # LOOV stats
    ridge_res_test <- c()
    
    for (i in 1:70) {
      
      # divide in train and test dataset
      j <- 1:70
      x_train <- x[j[-i],]
      x_test <- x[j[i],, drop=F]
      y_train <- y[j[-i],]
      y_test <- y[j[i],]
      
      # train ridge
      ridge <- glmnet(x_train,y_train,alpha=0)
      
      # calcualte prediction and residual
      ridge_res_test_tmp <- as.data.frame(predict(ridge, s=ridge_bestlam, newx=x_test))
      ridge_res_test <- append(ridge_res_test, ridge_res_test_tmp[1,1])
    }
    
    # TRAIN stats
    ridge  <- glmnet(x, y, alpha=0, lambda = ridge_bestlam)
    
    ridge_res <- as.data.frame(predict(ridge, s=ridge_bestlam, newx=x))
    ridge_res$res <- ridge_res[,1]
    
    # APPEND STATS
    row <- data.frame(
      model = 'ridge', 
      rmse_valid = rmse(y, ridge_res_test),
      mae_valid = mae(y, ridge_res_test),
      r2_valid = r2(y, ridge_res_test),
      rmse_train = rmse(y, ridge_res$res), 
      mae_train = mae(y, ridge_res$res),
      r2_train = r2(y,ridge_res$res)
    )
    
    model_stats <- rbind(model_stats[model_stats$model != 'ridge_rest', ], row)
    model_stats
  }

  # STEP 3 - LASSO
  {
    y <- as.matrix(data_model[,1])
    x <- as.matrix(data_model[,-1])
    
    # cross-validation and associated test error
    set.seed(435632)
    cv.out=cv.glmnet(x,y,nfold=70,alpha=1,grouped=FALSE)
    plot(cv.out)
    lasso_bestlam=cv.out$lambda.min
    lasso_bestlam
    
    # LOOV stats
    lasso_res_test <- c()
    
    for (i in 1:70) {
      
      # divide in train and test dataset
      j <- 1:70
      x_train <- x[j[-i],]
      x_test <- x[j[i],, drop=F]
      y_train <- y[j[-i],]
      y_test <- y[j[i],]
      
      # train lasso
      lasso <- glmnet(x_train,y_train,alpha=1)
      
      # calcualte prediction and residual
      lasso_res_test_tmp <- as.data.frame(predict(lasso, s=lasso_bestlam, newx=x_test))
      lasso_res_test <- append(lasso_res_test, lasso_res_test_tmp[1,1])
    }
    
    # TRAIN stats
    lasso  <- glmnet(x, y, alpha=1, lambda = lasso_bestlam)
    
    coef(lasso)
    
    lasso_res <- as.data.frame(predict(lasso, s=lasso_bestlam, newx=x))
    lasso_res$res <- lasso_res[,1]
    
    # APPEND STATS
    row <- data.frame(
      model = 'lasso', 
      rmse_valid = rmse(y, lasso_res_test),
      mae_valid = mae(y, lasso_res_test),
      r2_valid = r2(y, lasso_res_test),
      rmse_train = rmse(y, lasso_res$res), 
      mae_train = mae(y, lasso_res$res),
      r2_train = r2(y,lasso_res$res)
    )
    model_stats <- rbind(model_stats, row)
    model_stats
  }

  # STEP 4 - ELASTIC NET
  {
    y <- as.matrix(data_model[,1])
    x <- as.matrix(data_model[,-1])
    
    dataset <- as.matrix(cbind(y,x))
    colnames(dataset)[1] <- 'y'
    set.seed(435632)
    cv=trainControl(method="LOOCV")
    fit_elnet_int=train(y~.,data=dataset,method="glmnet",trControl=cv,tuneLength=10)
    # a helper function to extract the row with the best tuning parameters
    get_best_result = function(caret_fit) {
      best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
      best_result = caret_fit$results[best, ]
      rownames(best_result) = NULL
      best_result
    }
    elastic_full_bestalpha <- get_best_result(fit_elnet_int)[1]
    
    # cross-validation and associated test error
    set.seed(435632)
    cv.out=cv.glmnet(x,y,nfold=70,alpha=elastic_full_bestalpha,grouped=FALSE)
    plot(cv.out)
    elastic_bestlam=cv.out$lambda.min
    elastic_bestlam
    
    # LOOV stats
    elastic_res_test <- c()
    
    for (i in 1:70) {
      
      # divide in train and test dataset
      j <- 1:70
      x_train <- x[j[-i],]
      x_test <- x[j[i],, drop=F]
      y_train <- y[j[-i],]
      y_test <- y[j[i],]
      
      # train elastic
      elastic <- glmnet(x_train,y_train,alpha=1)
      
      # calcualte prediction and residual
      elastic_res_test_tmp <- as.data.frame(predict(elastic, s=elastic_bestlam, newx=x_test))
      elastic_res_test <- append(elastic_res_test, elastic_res_test_tmp[1,1])
    }
    
    # TRAIN stats
    elastic  <- glmnet(x, y, alpha=1, lambda = elastic_bestlam)
    
    elastic_res <- as.data.frame(predict(elastic, s=elastic_bestlam, newx=x))
    elastic_res$res <- elastic_res[,1]
    
    # APPEND STATS
    row <- data.frame(
      model = 'elastic', 
      rmse_valid = rmse(y, elastic_res_test),
      mae_valid = mae(y, elastic_res_test),
      r2_valid = r2(y, elastic_res_test),
      rmse_train = rmse(y, elastic_res$res), 
      mae_train = mae(y, elastic_res$res),
      r2_train = r2(y,elastic_res$res)
    )
    model_stats <- rbind(model_stats, row)
    model_stats 
  }

  # STEP 5 - OLS
  {
    # TRAIN stats
    ols <- lm(check_in_2019~.,data=data_model)
    summary(ols)
    
    # let's only leave significant variables
    ols_sig <- lm(check_in_2019 ~ dist + nursery + pop_density + train, data=data_model)
    summary(ols_sig)
    
    # no significant improvment
    anova(ols_sig, ols)
    # ols_sig will be the final model
    
    ols_res <- as.data.frame(predict(ols_sig))
    ols_res$res <- ols_res[,1]
    
    # LOOV stats
    ols_res_test <- c()
    
    for (i in 1:70) {
      
      # divide in train and test dataset
      x <- 1:70
      data_model_train <- data_model[x[-i],]
      data_model_test <- data_model[x[i],]
      
      # train ols
      ols <- lm(check_in_2019 ~ dist + nursery + pop_density + train, data=data_model_train)
      
      # calcualte prediction and residual
      ols_res_test_tmp <- as.data.frame(predict(ols, data_model_test[2:length(data_model_test)]))
      ols_res_test <- append(ols_res_test, ols_res_test_tmp[1,1])
    }
    
    row <- data.frame(
      model = 'ols', 
      rmse_valid = rmse(data_model$check_in_2019, ols_res_test),
      mae_valid = mae(data_model$check_in_2019, ols_res_test),
      r2_valid = r2(data_model$check_in_2019, ols_res_test),
      rmse_train = rmse(data_model$check_in_2019, ols_res$res), 
      mae_train = mae(data_model$check_in_2019, ols_res$res),
      r2_train = r2(data_model$check_in_2019, ols_res$res)
    )
    model_stats <- rbind(model_stats[model_stats$model != 'ols_2', ], row)
    model_stats
  }

  # STEP 6 - SVR
  {
    # Hyperparameters
    kernels <- c('linear', 'polynomial', 'radial', 'sigmoid')
    
    degrees <- 2:4
    
    # inverse of the standard deviation of the RBF kernel (Gaussian function)
    # small -> large variance; large -> small variance
    # larger variance means that points will be considered 'similar' more strictly
    gammas <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)  # 0-10,000
    
    # eplison parameter - error used in constraint
    es <- c(0.0001, 0.001, 0.01, 0.1, 1)
    
    # tolerance parameter - tolerance for points falling outside of |pred - epsilon|
    cs <- c(0.01, 0.1, 1, 10, 50, 100)
    
    # LEAVE ONE OUT VALIDATION
    
    svr_cv <- data.frame(
      kernel = character(), 
      eplison = numeric(),
      cost = numeric(),
      gamma = numeric(),
      degree = numeric(), 
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    eq <- check_in_2019 ~ .
    
    for (kernel in kernels) {
      print(kernel)
      for (e in es) {
        for (c in cs) {
          if (kernel == 'linear') {
            
            svr_res_test <- c()
            
            for (i in 1:70) {
              
              # divide in train and test dataset
              x <- 1:70
              data_model_train <- data_model[x[-i],]
              data_model_test <- data_model[x[i],]
              
              # train svr
              svr <- svm(eq, data=data_model_train, kernel = kernel, epsilon = e, cost = c)
              
              # calcualte prediction and residual
              svr_res_test_tmp <- as.data.frame(predict(svr, data_model_test[2:length(data_model_test)]))
              svr_res_test <- append(svr_res_test, svr_res_test_tmp[1,1])
            }
            
            # append results
            row <- data.frame(
              kernel = kernel, 
              epsilon = e,
              cost = c,
              gamma = NA,
              degree = NA,
              rmse = rmse(data_model$check_in_2019, svr_res_test),
              mae = mae(data_model$check_in_2019, svr_res_test),
              r2 = r2(data_model$check_in_2019, svr_res_test),
              r2_adj = r2_adj(data_model$check_in_2019, svr_res_test)
            )
            
            svr_cv <- rbind(svr_cv, row)
            
          } else {
            for (gamma in gammas) {
              if (kernel == 'polynomial') {
                for (degree in degrees) {
                  
                  svr_res_test <- c()
                  
                  for (i in 1:70) {
                    
                    # divide in train and test dataset
                    x <- 1:70
                    data_model_train <- data_model[x[-i],]
                    data_model_test <- data_model[x[i],]
                    
                    # train svr
                    svr <- svm(eq, data=data_model_train, kernel = kernel, epsilon = e, cost = c, gamma = gamma, degree = degree)
                    
                    # calcualte prediction and residual
                    svr_res_test_tmp <- as.data.frame(predict(svr, data_model_test[2:length(data_model_test)]))
                    svr_res_test <- append(svr_res_test, svr_res_test_tmp[1,1])
                    
                  }
                  
                  # append results
                  row <- data.frame(
                    kernel = kernel, 
                    epsilon = e,
                    cost = c,
                    gamma = gamma,
                    degree = degree,
                    rmse = rmse(data_model$check_in_2019, svr_res_test),
                    mae = mae(data_model$check_in_2019, svr_res_test),
                    r2 = r2(data_model$check_in_2019, svr_res_test),
                    r2_adj = r2_adj(data_model$check_in_2019, svr_res_test)
                  )
                  
                  svr_cv <- rbind(svr_cv, row)
                }
              } else {
                
                svr_res_test <- c()
                
                for (i in 1:70) {
                  
                  # divide in train and test dataset
                  x <- 1:70
                  data_model_train <- data_model[x[-i],]
                  data_model_test <- data_model[x[i],]
                  
                  # train svr
                  svr <- svm(eq, data=data_model_train, kernel = kernel, epsilon = e, cost = c, gamma = gamma)
                  
                  # calcualte prediction and residual
                  svr_res_test_tmp <- as.data.frame(predict(svr, data_model_test[2:length(data_model_test)]))
                  svr_res_test <- append(svr_res_test, svr_res_test_tmp[1,1])
                  
                }
                
                # append results
                row <- data.frame(
                  kernel = kernel, 
                  epsilon = e,
                  cost = c,
                  gamma = gamma,
                  degree = NA,
                  rmse = rmse(data_model$check_in_2019, svr_res_test),
                  mae = mae(data_model$check_in_2019, svr_res_test),
                  r2 = r2(data_model$check_in_2019, svr_res_test),
                  r2_adj = r2_adj(data_model$check_in_2019, svr_res_test)
                )
                
                svr_cv <- rbind(svr_cv, row)
              }
            }
          }
        }
      }
    }
    
    svr_cv[svr_cv$rmse == min (svr_cv$rmse) | svr_cv$mae == min (svr_cv$mae) | svr_cv$r2 == max (svr_cv$r2), ]
    
    kernels <- c('radial')
    
    # inverse of the standard deviation of the RBF kernel (Gaussian function)
    # small -> large variance; large -> small variance
    # larger variance means that points will be considered 'similar' more strictly
    gammas <- 0.002
    
    # eplison parameter - error used in constraint
    es <- 0.13
    
    # tolerance parameter - tolerance for points falling outside of |pred - epsilon|
    cs <- 201:219
    
    # LEAVE ONE OUT VALIDATION
    
    svr_cv_2 <- data.frame(
      kernel = character(), 
      eplison = numeric(),
      cost = numeric(),
      gamma = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (e in es) {
      print(e)
      for (c in cs) {
        for (gamma in gammas) {
          for (kernel in kernels) {
            
            svr_res_test <- c()
            
            for (i in 1:70) {
              
              # divide in train and test dataset
              x <- 1:70
              data_model_train <- data_model[x[-i],]
              data_model_test <- data_model[x[i],]
              
              # train svr
              svr <- svm(eq, data=data_model_train, kernel = kernel, epsilon = e, cost = c, gamma = gamma)
              
              # calcualte prediction and residual
              svr_res_test_tmp <- as.data.frame(predict(svr, data_model_test))
              svr_res_test <- append(svr_res_test, svr_res_test_tmp[1,1])
            }
            
            # append results
            row <- data.frame(
              kernel = kernel, 
              epsilon = e,
              cost = c,
              gamma = gamma,
              rmse = rmse(data_model$check_in_2019, svr_res_test),
              mae = mae(data_model$check_in_2019, svr_res_test),
              r2 = r2(data_model$check_in_2019, svr_res_test),
              r2_adj = r2_adj(data_model$check_in_2019, svr_res_test)
            )
            
            svr_cv_2 <- rbind(svr_cv_2, row)
          }
        }
      }
    }
    
    # SELECT BEST PERFORMING MODEL
    svr_cv_2[svr_cv_2$rmse == min (svr_cv_2$rmse) | svr_cv_2$mae == min (svr_cv_2$mae) | svr_cv_2$r2 == max (svr_cv_2$r2), ]
    
    svr <- svm(eq, data=data_model, kernel='radial', epsilon = 0.13, cost = 207, gamma = 0.002)
    
    svr_res <- as.data.frame(predict(svr, data_model))
    svr_res$res <- svr_res[,1]
    
    rmse(data_model$check_in_2019, svr_res$res)
    mae(data_model$check_in_2019, svr_res$res)
    r2(data_model$check_in_2019, svr_res$res)
    
    # APPEND STATS
    row <- data.frame(
      model = 'svr', 
      rmse_valid = svr_cv_2[svr_cv_2$mae == min (svr_cv_2$mae), 'rmse'],
      mae_valid =svr_cv_2[svr_cv_2$mae == min (svr_cv_2$mae), 'mae'],
      r2_valid = svr_cv_2[svr_cv_2$mae == min (svr_cv_2$mae), 'r2'],
      rmse_train = rmse(data_model$check_in_2019, svr_res$res), 
      mae_train = mae(data_model$check_in_2019, svr_res$res),
      r2_train = r2(data_model$check_in_2019, svr_res$res)
    )
    model_stats <- rbind(model_stats, row)
    model_stats
    
    # RESULTS PLOT
    svr_res_graph <- data.frame(
      borough = panel_metrics$borough, 
      actual = data_model$check_in_2019, 
      pred = round(svr_res$res, 0)
    )
    
    svr_res_graph$actual_vs_pred <- ifelse(svr_res_graph$actual > svr_res_graph$pred, 'higher', 'lower')
    
    svr_plot <- ggplot(svr_res_graph, aes(x = borough, y = actual)) +
      geom_bar(col = 'blue', stat='identity', width = 1) +
      geom_bar(data = svr_res_graph, aes(x = borough, y = pred, col = actual_vs_pred), stat='identity', width=0.2) + 
      theme(
        plot.title = element_text(color="#000000", face="bold", size=24, hjust=0.5),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
      ggtitle('SVR Results')
  }

  # STEP 7 - RANDOM FOREST
  {
    # HYPERPARAMETERS
    ntrees <- 10 * 1:15
    
    # no of variables used in a tree
    mtrys <- c(3, 5, 10, 15, 20)
    
    # min number of observations in a terminal node
    nodesizes <- c(5, 10, 15, 20)
    
    # max number of terminal nodes (NULL means that the tree is grown to max possible extent)
    maxnodes <- c(3, 5, 10)
    
    rf_cv <- data.frame(
      ntree = numeric(), 
      mtry = numeric(),
      nodesize = numeric(),
      maxnode = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (ntree in ntrees) {
      print(ntree)
      for (mtry in mtrys) {
        for (nodesize in nodesizes) {
          for (maxnode in maxnodes) {
            
            rf_res_test <- c()
            
            for (i in 1:70) {
              
              # divide in train and test dataset
              x <- 1:70
              data_model_train <- data_model[x[-i],]
              data_model_test <- data_model[x[i],]
              
              # train rf
              set.seed(435632)
              rf <- randomForest(check_in_2019~., data=data_model_train, ntree = ntree, mtry = mtry, replace = F, nodesize = nodesize, maxnode = maxnode)
              
              # calcualte prediction and residual
              rf_res_test_tmp <- as.data.frame(predict(rf, data_model_test[2:length(data_model_test)]))
              rf_res_test <- append(rf_res_test, rf_res_test_tmp[1,1])
              
            }
            # append results
            row <- data.frame(
              ntree = ntree, 
              mtry = mtry,
              nodesize = nodesize,
              maxnode = maxnode,
              rmse = rmse(data_model$check_in_2019, rf_res_test),
              mae = mae(data_model$check_in_2019, rf_res_test),
              r2 = r2(data_model$check_in_2019, rf_res_test),
              r2_adj = r2_adj(data_model$check_in_2019, rf_res_test)
            )
            
            rf_cv <- rbind(rf_cv, row)
          }
        }
      }
    }
    
    rf_cv[rf_cv$rmse == min (rf_cv$rmse) | rf_cv$mae == min (rf_cv$mae) | rf_cv$r2 == max (rf_cv$r2), ]
    
    ntrees <- 15:19
    
    # no of variables used in a tree
    mtrys <- 7:13
    
    # min number of observations in a terminal node
    nodesizes <- 9:15
    
    # max number of terminal nodes (NULL means that the tree is grown to max possible extent)
    maxnodes <- 4:9
    
    rf_cv_2 <- data.frame(
      ntree = numeric(), 
      mtry = numeric(),
      nodesize = numeric(),
      maxnode = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (ntree in ntrees) {
      print(ntree)
      for (mtry in mtrys) {
        for (nodesize in nodesizes) {
          for (maxnode in maxnodes) {
            
            rf_res_test <- c()
            
            for (i in 1:70) {
              
              # divide in train and test dataset
              x <- 1:70
              data_model_train <- data_model[x[-i],]
              data_model_test <- data_model[x[i],]
              
              # train rf
              set.seed(435632)
              rf <- randomForest(check_in_2019~., data=data_model_train, ntree = ntree, mtry = mtry, replace = F, nodesize = nodesize, maxnode = maxnode)
              
              # calcualte prediction and residual
              rf_res_test_tmp <- as.data.frame(predict(rf, data_model_test[2:length(data_model_test)]))
              rf_res_test <- append(rf_res_test, rf_res_test_tmp[1,1])
              
            }
            # append results
            row <- data.frame(
              ntree = ntree, 
              mtry = mtry,
              nodesize = nodesize,
              maxnode = maxnode,
              rmse = rmse(data_model$check_in_2019, rf_res_test),
              mae = mae(data_model$check_in_2019, rf_res_test),
              r2 = r2(data_model$check_in_2019, rf_res_test),
              r2_adj = r2_adj(data_model$check_in_2019, rf_res_test)
            )
            
            rf_cv_2 <- rbind(rf_cv_2, row)
          }
        }
      }
    }
    
    # SELECT BEST PERFORMING MODEL
    rf_cv_2[rf_cv_2$rmse == min (rf_cv_2$rmse) | rf_cv_2$mae == min (rf_cv_2$mae) | rf_cv_2$r2 == max (rf_cv_2$r2), ]
    
    set.seed(435632)
    rf <- randomForest(check_in_2019~., data=data_model, importance = T, ntree=17, mtry=10, replace=F, nodesize=12, maxnode=7)
    importance(rf)
    #rf
    #varImpPlot(rf)
    
    rf_res <- as.data.frame(predict(rf, data_model[2:length(data_model)]))
    rf_res$res <- rf_res[,1]
    
    rmse(data_model$check_in_2019, rf_res$res)
    mae(data_model$check_in_2019, rf_res$res)
    r2(data_model$check_in_2019, rf_res$res)
    
    # APPEND STATS
    row <- data.frame(
      model = 'rf', 
      rmse_valid = rf_cv_2[rf_cv_2$mae == min (rf_cv_2$mae), 'rmse'],
      mae_valid =rf_cv_2[rf_cv_2$mae == min (rf_cv_2$mae), 'mae'],
      r2_valid = rf_cv_2[rf_cv_2$mae == min (rf_cv_2$mae), 'r2'],
      rmse_train = rmse(data_model$check_in_2019, rf_res$res), 
      mae_train = mae(data_model$check_in_2019, rf_res$res),
      r2_train = r2(data_model$check_in_2019, rf_res$res)
    )
    model_stats <- rbind(model_stats[model_stats$model != 'rf',], row)
    model_stats
    
    # PLOT RESULTS
    rf_res_graph <- data.frame(
      borough = panel_metrics$borough, 
      actual = data_model$check_in_2019, 
      pred = round(rf_res$res, 0)
    )
    
    rf_res_graph$actual_vs_pred <- ifelse(rf_res_graph$actual > rf_res_graph$pred, 'higher', 'lower')
    
    rf_plot <- ggplot(rf_res_graph, aes(x = borough, y = actual)) +
      geom_bar(col = 'blue', stat='identity', width = 1) +
      geom_bar(data = rf_res_graph, aes(x = borough, y = pred, col = actual_vs_pred), stat='identity', width=0.2) + 
      theme(
        plot.title = element_text(color="#000000", face="bold", size=24, hjust=0.5),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
      ggtitle('RF Results')
  }

  # STEP 8 - XGBOOST
  {
    # HYPERPARAMETERS
    # first learning rate & iteration & after that tree-specific parameters & after that boosting parameters again
    
    # number of iterations
    nroundss <- 3:7
    
    booster <- 'gbtree'
    
    # learning rate - shrinking the feature weights after each iteration to prevent overfitting
    etas <- 0.05 * 1:19
    
    # minimum loss reduction required to make a further partition on a leaf node of the tree
    gammas <- c(0, 10, 100, 1000, 5000, 10000, 50000, 100000)
    
    # tree concstruction - since our sample is very small we will use 'exact' (the greedy appraoch)
    tree_method <- 'exact'
    
    xgb_gbtree_cv_1 <- data.frame(
      booster = character(),
      nrounds = numeric(),
      eta = numeric(), 
      gamma = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (eta in etas) {
      print(eta)
      for (gamma in gammas) {
        
        params <- list(
          booster = booster,
          eta = eta,
          gamma = gamma,
          tree_method = tree_method,
          objective = "reg:squarederror",
          eval_metric = "rmse")
        
        for (nrounds in nroundss) {
          
          xgb_res_test <- c()
          
          for (i in 1:70) {
            
            # divide in train and test dataset
            x <- 1:70
            data_model_train <- data_model[x[-i],]
            data_model_test <- data_model[x[i],]
            
            data_model_train_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_train[,-1]), label = data_model_train[,1])
            
            data_model_test_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_test[,-1]), label = data_model_test[,1])
            
            # train model
            set.seed(435632)
            xgb <- xgb.train(params = params, data = data_model_train_dmatrix, nrounds = nrounds, verbose = 0)
            
            # calcualte prediction and residual
            xgb_res_test_tmp <- as.data.frame(predict(xgb, data_model_test_dmatrix))
            xgb_res_test <- append(xgb_res_test, xgb_res_test_tmp[1,1])
          }
          
          # append results
          row <- data.frame(
            booster = 'gbtree',
            nrounds = nrounds,
            eta = eta, 
            gamma = gamma,
            rmse = rmse(data_model$check_in_2019, xgb_res_test),
            mae = mae(data_model$check_in_2019, xgb_res_test),
            r2 = r2(data_model$check_in_2019, xgb_res_test),
            r2_adj = r2_adj(data_model$check_in_2019, xgb_res_test)
          )
          
          xgb_gbtree_cv_1 <- rbind(xgb_gbtree_cv_1, row)
        }            
      }
    }
    
    xgb_gbtree_cv_1[xgb_gbtree_cv_1$rmse == min (xgb_gbtree_cv_1$rmse) | xgb_gbtree_cv_1$mae == min (xgb_gbtree_cv_1$mae) | xgb_gbtree_cv_1$r2 == max (xgb_gbtree_cv_1$r2), ]
    
    # number of iterations
    nrounds <- 7
    
    booster <- 'gbtree'
    
    # learning rate - shrinking the feature weights after each iteration to prevent overfitting
    eta <- 0.4
    
    # minimum loss reduction required to make a further partition on a leaf node of the tree
    gamma <- 100
    
    # L2 regularization param 
    lambdas <- c(0, 1, 10, 100, 1000, 10000, 100000)
    
    # L1 regularization param 
    alphas <- c(0, 1, 10, 100, 1000, 10000, 100000)
    
    # tree concstruction - since our sample is very small we will use 'exact' (the greedy appraoch)
    tree_method <- 'exact'
    
    xgb_gbtree_cv_2 <- data.frame(
      lambda = character(),
      alpha = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (lambda in lambdas) {
      print(lambda)
      for (alpha in alphas) {
        
        params <- list(
          booster = booster,
          eta = eta,
          gamma = gamma,
          alpha = alpha,
          lambda = lambda,
          tree_method = tree_method,
          objective = "reg:squarederror",
          eval_metric = "rmse")
        
        xgb_res_test <- c()
        
        for (i in 1:70) {
          
          # divide in train and test dataset
          x <- 1:70
          data_model_train <- data_model[x[-i],]
          data_model_test <- data_model[x[i],]
          
          data_model_train_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_train[,-1]), label = data_model_train[,1])
          
          data_model_test_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_test[,-1]), label = data_model_test[,1])
          
          # train model
          set.seed(435632)
          xgb <- xgb.train(params = params, data = data_model_train_dmatrix, nrounds = nrounds, verbose = 0)
          
          # calcualte prediction and residual
          xgb_res_test_tmp <- as.data.frame(predict(xgb, data_model_test_dmatrix))
          xgb_res_test <- append(xgb_res_test, xgb_res_test_tmp[1,1])
        }
        
        # append results
        row <- data.frame(
          lambda = lambda,
          alpha = alpha,
          rmse = rmse(data_model$check_in_2019, xgb_res_test),
          mae = mae(data_model$check_in_2019, xgb_res_test),
          r2 = r2(data_model$check_in_2019, xgb_res_test),
          r2_adj = r2_adj(data_model$check_in_2019, xgb_res_test)
        )
        
        xgb_gbtree_cv_2 <- rbind(xgb_gbtree_cv_2, row)
      }
    }
    
    xgb_gbtree_cv_2[xgb_gbtree_cv_2$rmse == min (xgb_gbtree_cv_2$rmse) | xgb_gbtree_cv_2$mae == min (xgb_gbtree_cv_2$mae) | xgb_gbtree_cv_2$r2 == max (xgb_gbtree_cv_2$r2), ]
    
    # number of iterations
    nroundss <- 7
    
    booster <- 'gbtree'
    
    # learning rate - shrinking the feature weights after each iteration to prevent overfitting
    etas <- 0.4
    
    # minimum loss reduction required to make a further partition on a leaf node of the tree
    gamma <- 100
    
    # L2 regularization param 
    lambda <- 0
    
    # L1 regularization param 
    alpha <- 100
    
    # max tree depth
    max_depths <- 2:8
    
    # minimum number of instances needed to be in each node
    min_child_weights <- c(7, 10, 12, 15, 20)
    
    # data subsample for each iteration
    subsamples <- 0.2 * 1:5
    
    # ratio of columns for each tree
    colsamples_bytree <- 0.2 * 1:5
    
    # tree concstruction - since our sample is very small we will use 'exact' (the greedy appraoch)
    tree_method <- 'exact'
    
    xgb_gbtree_cv_3 <- data.frame(
      nrounds = numeric(),
      eta = numeric(),
      max_depths = numeric(),
      min_child_weights = numeric(),
      subsamples = numeric(),
      colsamples_bytree = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (max_depth in max_depths) {
      print(max_depth)
      for (min_child_weight in min_child_weights) {
        for (subsample in subsamples) {
          for (colsample_bytree in colsamples_bytree) {
            for (nrounds in nroundss) {
              for (eta in etas) {
                params <- list(
                  booster = booster,
                  eta = eta,
                  gamma = gamma,
                  alpha = alpha,
                  lambda = lambda,
                  max_depth = max_depth,
                  min_child_weight = min_child_weight,
                  subsample = subsample,
                  colsample_bytree = colsample_bytree,
                  tree_method = tree_method,
                  objective = "reg:squarederror",
                  eval_metric = "rmse")
                
                xgb_res_test <- c()
                
                for (i in 1:70) {
                  
                  # divide in train and test dataset
                  x <- 1:70
                  data_model_train <- data_model[x[-i],]
                  data_model_test <- data_model[x[i],]
                  
                  data_model_train_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_train[,-1]), label = data_model_train[,1])
                  
                  data_model_test_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_test[,-1]), label = data_model_test[,1])
                  
                  # train model
                  set.seed(435632)
                  xgb <- xgb.train(params = params, data = data_model_train_dmatrix, nrounds = nrounds, verbose = 0)
                  
                  # calcualte prediction and residual
                  xgb_res_test_tmp <- as.data.frame(predict(xgb, data_model_test_dmatrix))
                  xgb_res_test <- append(xgb_res_test, xgb_res_test_tmp[1,1])
                }
                
                # append results
                row <- data.frame(
                  nrounds = nrounds,
                  eta = eta,
                  max_depths = max_depth,
                  min_child_weights = min_child_weight,
                  subsamples = subsample,
                  colsamples_bytree = colsample_bytree,
                  rmse = rmse(data_model$check_in_2019, xgb_res_test),
                  mae = mae(data_model$check_in_2019, xgb_res_test),
                  r2 = r2(data_model$check_in_2019, xgb_res_test),
                  r2_adj = r2_adj(data_model$check_in_2019, xgb_res_test)
                )
                
                xgb_gbtree_cv_3 <- rbind(xgb_gbtree_cv_3, row)
              }  
            }
          }
        }
      }
    }
    
    xgb_gbtree_cv_3[xgb_gbtree_cv_3$rmse == min (xgb_gbtree_cv_3$rmse) | xgb_gbtree_cv_3$mae == min (xgb_gbtree_cv_3$mae) | xgb_gbtree_cv_3$r2 == max (xgb_gbtree_cv_3$r2), ]
    
    # number of iterations
    nroundss <- 8
    
    booster <- 'gbtree'
    
    # learning rate - shrinking the feature weights after each iteration to prevent overfitting
    etas <- 0.36
    
    # minimum loss reduction required to make a further partition on a leaf node of the tree
    gammas <- 0
    
    # L2 regularization param 
    lambdas <- 0
    
    # L1 regularization param 
    alphas <- 10 * 1:10
    
    # max tree depth
    max_depth <- 3
    
    # minimum number of instances needed to be in each node
    min_child_weight <- 7
    
    # data subsample for each iteration
    subsample <- 0.8
    
    # ratio of columns for each tree
    colsample_bytree <- 0.4
    
    # tree concstruction - since our sample is very small we will use 'exact' (the greedy appraoch)
    tree_method <- 'exact'
    
    xgb_gbtree_cv_4 <- data.frame(
      booster = character(),
      nrounds = numeric(),
      eta = numeric(), 
      gamma = numeric(),
      lambda = numeric(),
      alpha = numeric(),
      rmse = numeric(),
      mae = numeric(),
      r2 = numeric(),
      r2_adj = numeric()
    )
    
    for (eta in etas) {
      print(eta)
      for (gamma in gammas) {
        for (lambda in lambdas) {
          for (alpha in alphas) {
            
            params <- list(
              booster = booster,
              eta = eta,
              gamma = gamma,
              lambda = lambda,
              alpha = alpha,
              max_depth = max_depth,
              min_child_weight = min_child_weight,
              subsample = subsample,
              colsample_bytree = colsample_bytree,
              tree_method = tree_method,
              objective = "reg:squarederror",
              eval_metric = "rmse")
            
            for (nrounds in nroundss) {
              
              xgb_res_test <- c()
              
              for (i in 1:70) {
                
                # divide in train and test dataset
                x <- 1:70
                data_model_train <- data_model[x[-i],]
                data_model_test <- data_model[x[i],]
                
                data_model_train_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_train[,-1]), label = data_model_train[,1])
                
                data_model_test_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_test[,-1]), label = data_model_test[,1])
                
                # train model
                set.seed(435632)
                xgb <- xgb.train(params = params, data = data_model_train_dmatrix, nrounds = nrounds, verbose = 0)
                
                # calcualte prediction and residual
                xgb_res_test_tmp <- as.data.frame(predict(xgb, data_model_test_dmatrix))
                xgb_res_test <- append(xgb_res_test, xgb_res_test_tmp[1,1])
              }
              
              # append results
              row <- data.frame(
                booster = 'gbtree',
                nrounds = nrounds,
                eta = eta, 
                gamma = gamma,
                lambda = lambda,
                alpha = alpha,
                rmse = rmse(data_model$check_in_2019, xgb_res_test),
                mae = mae(data_model$check_in_2019, xgb_res_test),
                r2 = r2(data_model$check_in_2019, xgb_res_test),
                r2_adj = r2_adj(data_model$check_in_2019, xgb_res_test)
              )
              
              xgb_gbtree_cv_4 <- rbind(xgb_gbtree_cv_4, row)
            }
          }
        }            
      }
    }
    
    # SELECT BEST PERFORMING MODEL
    xgb_gbtree_cv_4[xgb_gbtree_cv_4$rmse == min (xgb_gbtree_cv_4$rmse) | xgb_gbtree_cv_4$mae == min (xgb_gbtree_cv_4$mae) | xgb_gbtree_cv_4$r2 == max (xgb_gbtree_cv_4$r2), ]
    
    params <- list(
      booster = 'gbtree',
      eta = 0.36,
      gamma = 0,
      alpha = 40,
      lambda = 0,
      max_depth = 3,
      min_child_weight = 7,
      subsample = 0.8,
      colsample_bytree = 0.4,
      tree_method = 'exact',
      objective = 'reg:squarederror',
      eval_metric = 'rmse')
    
    data_model_dmatrix <- xgb.DMatrix(data = data.matrix(data_model[,-1]), label = data_model[,1])
    
    set.seed(435632)
    xgb_gbtree <- xgb.train(params = params, data = data_model_dmatrix, nrounds = 8, verbose = 2)
    
    xgb_gbtree_res <- as.data.frame(predict(xgb_gbtree, data_model_dmatrix))
    xgb_gbtree_res$res <- xgb_gbtree_res[,1]
    
    rmse(data_model$check_in_2019, xgb_gbtree_res$res)
    mae(data_model$check_in_2019, xgb_gbtree_res$res)
    r2(data_model$check_in_2019, xgb_gbtree_res$res)
    
    # APPEND STATS
    row <- data.frame(
      model = 'xgb', 
      rmse_valid = xgb_gbtree_cv_4[xgb_gbtree_cv_4$rmse == min (xgb_gbtree_cv_4$rmse) & xgb_gbtree_cv_4$r2 == max (xgb_gbtree_cv_4$r2), 'rmse'],
      mae_valid = xgb_gbtree_cv_4[xgb_gbtree_cv_4$rmse == min (xgb_gbtree_cv_4$rmse) & xgb_gbtree_cv_4$r2 == max (xgb_gbtree_cv_4$r2), 'mae'],
      r2_valid = xgb_gbtree_cv_4[xgb_gbtree_cv_4$rmse == min (xgb_gbtree_cv_4$rmse) & xgb_gbtree_cv_4$r2 == max (xgb_gbtree_cv_4$r2), 'r2'],
      rmse_train = rmse(data_model$check_in_2019, xgb_gbtree_res$res), 
      mae_train = mae(data_model$check_in_2019, xgb_gbtree_res$res),
      r2_train = r2(data_model$check_in_2019, xgb_gbtree_res$res)
    )
    model_stats <- rbind(model_stats[model_stats$model != 'xgb_2',], row)
    model_stats
    
    write.xlsx(model_stats, 'data/csv/model_stats.xlsx')
    
    # PLOT RESULTS - TEST STATS
    xgb_gbtree_res_graph <- data.frame(
      borough = panel_metrics$borough, 
      actual = data_model$check_in_2019, 
      pred = round(xgb_res_test, 0)
    )
    
    xgb_gbtree_res_graph$actual_vs_pred <- ifelse(xgb_gbtree_res_graph$actual > xgb_gbtree_res_graph$pred, 'higher', 'lower')
    
    xgb_gbtree_plot <- ggplot(xgb_gbtree_res_graph, aes(x = borough, y = actual)) +
      geom_bar(col = 'blue', stat='identity', width = 1) +
      geom_bar(data = xgb_gbtree_res_graph, aes(x = borough, y = pred, col = actual_vs_pred), stat='identity', width=0.2) + 
      theme(
        plot.title = element_text(color="#000000", face="bold", size=24, hjust=0.5),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
      ggtitle('XGB Results')
  }

  
# PLOTS

# PLEASE NOTE THAT THE PLOTS SHOULD RUN RUN ONE AFTER ANOTHER AS THE FILES ARE LOADED ONLY ONCE
# Names of the important objects created in a plot are specified below the name

  # PLOT 1 - MAP WITH NAMES
  # objects: loc, bldg, hghws
  {
    # load polygon files created in previous steps
    locs <- readRDS('data/RDS/locs.RDS')
    bldgs <- readRDS('data/RDS/bldgs.RDS')
    hghws <- readRDS('data/RDS/hghws.RDS')
    
    # preapre polygon data
    locs_sp <- as(locs$osm_multipolygon, 'Spatial')
    
    # preapre data with polygons' centroids to add labels to map
    locs_cent_sp <- st_transform(locs$osm_multipolygon, crs = 2180) %>%
      st_geometry() %>%
      st_centroid() %>%
      st_transform(., crs ="+proj=longlat +datum=WGS84") %>%
      as(., 'Spatial') %>%
      as.data.frame() %>%
      mutate (
        borough = locs$osm_multipolygons$gmina,
        x = as.numeric(coords.x1),
        y = as.numeric(coords.x2)
      ) %>%
      dplyr::select(borough, x, y)
    
    # for some borughs, numbers will be used instead of names (for readability purposes)
    boroughs_numbers <- c('Nowy Dwór Mazowiecki', 'Piastów', 'Podkowa Leśna', 'Milanówek', 'Mińsk Mazowiecki 1', 'Mińsk Mazowiecki 2', 'Legionowo', 'Pruszków', 'Sulejówek')
    
    # filter for boroughs where numbers will be shown
    locs_cent_sp_number <- locs_cent_sp[locs_cent_sp$borough %in% boroughs_numbers, ]
    
    # add unique numbers to borough
    locs_cent_sp_number$number <- 1:nrow(locs_cent_sp_number)
    
    # create data frame to add the names of borough denoted with numbers
    (max(locs_cent_sp_name$y) - min(locs_cent_sp_name$y)) / 9 # -0.0709
    
    map_legend <- data.frame(
      number = locs_cent_sp_number$number,
      borough = locs_cent_sp_number$borough,
      x = max(locs_cent_sp_name$x) + 0.15,
      y = seq(max(locs_cent_sp_name$y), min(locs_cent_sp_name$y), -0.0709)
    )
    
    # filter for boroughs where names will be shown
    locs_cent_sp_name <- locs_cent_sp[!locs_cent_sp$borough %in% boroughs_numbers, ]
    
    # add enters to names that have more than one word (for readability purposes)
    for (i in (1:nrow(locs_cent_sp_name))) {
      name <- locs_cent_sp_name[i, 'borough']
      if (grepl(' ', name) ||  grepl('-', name)) {
        name <- ifelse(grepl(' ', name), strsplit(name, ' '), strsplit(name, '-'))[[1]]
        name <- paste(name,  collapse = '\n')
        locs_cent_sp_name[i, 'borough'] <- name
      }
    }
    
    # manually adjust locations to better fit the name
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Lesznowola', 'y'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Lesznowola', 'y'] + 0.01
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Ożarów\nMazowiecki', 'y'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Ożarów\nMazowiecki', 'y'] + 0.005
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Wiązowna', 'y'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Wiązowna', 'y'] + 0.01
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Wiązowna', 'x'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Wiązowna', 'x'] - 0.01
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Celestynów', 'x'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Celestynów', 'x'] - 0.01
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Pomiechówek', 'x'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Pomiechówek', 'x'] - 0.01
    locs_cent_sp_name[locs_cent_sp_name$borough == 'Pomiechówek', 'y'] <- locs_cent_sp_name[locs_cent_sp_name$borough == 'Pomiechówek', 'y'] - 0.014
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 2', 'y'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 2', 'y'] - 0.01
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 2', 'x'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 2', 'x'] + 0.045
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 1', 'y'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Mińsk Mazowiecki 1', 'y'] + 0.005
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Nowy Dwór Mazowiecki', 'y'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Nowy Dwór Mazowiecki', 'y'] - 0.011
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Nowy Dwór Mazowiecki', 'x'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Nowy Dwór Mazowiecki', 'x'] + 0.02
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Podkowa Leśna', 'y'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Podkowa Leśna', 'y'] - 0.007
    locs_cent_sp_number[locs_cent_sp_number$borough == 'Podkowa Leśna', 'x'] <- locs_cent_sp_number[locs_cent_sp_number$borough == 'Podkowa Leśna', 'x'] + 0.018
    
    # plot
    map_names <- ggplot(locs_sp, aes(x=long, y=lat, group=group)) +
      # borough outlines and fill
      geom_polygon(color = 'black', fill = '#f9f9f9', size=1) +
      # number labels on the map
      geom_label(
        data = locs_cent_sp_number,
        mapping = aes(x=x, y=y, label = number),
        size = 2.5,
        color = 'black',
        fontface = 'bold',
        inherit.aes = F) +
      # number labels on the legend
      geom_label(
        data = map_legend,
        mapping = aes(x=x, y=y, label = number),
        size = 2.5,
        color = 'black',
        fontface = 'bold',
        inherit.aes = F) +
      # text labels on the legend
      geom_text(
        data = map_legend,
        mapping = aes(x=x + 0.03, y=y, label = borough),
        size = 2.5,
        color = 'black',
        fontface = 'bold',
        hjust = 0,
        inherit.aes = F) +
      # empty points to fit the legend's text labels
      geom_point(
        data = map_legend,
        mapping = aes(x=x + 0.15, y=y),
        size = 0,
        color = NA,
        inherit.aes = F) +
      # text labels on the map
      geom_text(
        data = locs_cent_sp_name,
        mapping = aes(x=x, y=y, label = borough),
        size = 2,
        color = 'black',
        fontface = 'bold',
        angle = 20,
        inherit.aes = F) +
      coord_fixed() +
      theme_void()
    
    # save plot
    png('img/map_names.png', width = 3508, height = 1700, res=300)
    map_names
    dev.off()
  }
  
  # PLOT 2 - MAP WITH BORDERS, HOUSES & ROADS
  # objects: hghws_lrg
  {
    # large highway types
    hghws_tp_lrg <- c('motorway', 'trunk', 'primary', 'secondary', 'tertiary')
    
    # select only large highways
    hghws_lrg <- hghws$osm_lines[hghws$osm_lines$highway %in% hghws_tp_lrg,]
    
    c <- list()
    
    c <- append(c,'')
    
    # function to plot boroughs_osm
    plot_loc <- function(borough = 'all', l=T, b=T, h=T, hl=T) {
      # create spatial object for loc, bldgs, hghws based on selection go borough
      if (borough=='all') {
        locs_sp <- as(locs$osm_multipolygon, 'Spatial')
        bldgs_sp <- as(bldgs$osm_polygons, 'Spatial')
        hghws_sp <- as(hghws$osm_lines, 'Spatial')
        hghws_lrg_sp <- as(hghws_lrg, 'Spatial')
      } else if (class(borough) == 'character') {
        locs_sp <- as(locs$osm_multipolygon[locs$osm_multipolygon$borough %in% borough,], 'Spatial')
        bldgs_sp <- as(bldgs$osm_polygons[bldgs$osm_polygons$borough %in% borough,], 'Spatial')
        hghws_sp <- as(hghws$osm_lines[hghws$osm_lines$borough %in% borough,], 'Spatial')
        hghws_lrg_sp <- as(hghws_lrg[hghws_lrg$borough %in% borough,], 'Spatial')
      } else if (class(borough) == 'numeric') {
        locs_sp <- as(locs$osm_multipolygon[borough,], 'Spatial')
        bldgs_sp <- as(bldgs$osm_polygons[borough,], 'Spatial')
        hghws_sp <- as(hghws$osm_lines[borough,], 'Spatial')
        hghws_lrg_sp <- as(hghws_lrg[borough,], 'Spatial')
      }
      
      legend <- c()
      if (l) {legend <- append(legend, c('border' = 'black'))}
      if (b) {legend <- append(legend, c('building' = '#482677'))}
      if (hl) {legend <- append(legend, c('large road' = '#1F968B'))}
      if (h) {legend <- append(legend, c('road' = '#73D055'))}
      
      # plot
      p <- ggplot() +
        {if (h) geom_line(data = hghws_sp, aes(x=long, y=lat, group=group, colour = 'road'), size=0.2)} +
        {if (hl) geom_line(data = hghws_lrg_sp, aes(x=long, y=lat, group=group, colour = 'large road'), size=0.4)} +
        {if (b) geom_polygon(data = bldgs_sp, aes(x=long, y=lat, group=group, colour = 'building'), fill = '#482677', size=0)} +
        {if (l) geom_polygon(data = locs_sp, aes(x=long, y=lat, group=group, colour = 'border'), fill=NA, size=1)} +
        labs(color = NULL) +
        scale_color_manual(values = legend) +
        theme_void() +
        coord_fixed() +
        theme(
          legend.position = 'top',
          legend.text = element_text(size=10)
          ) +
        guides(color = guide_legend(
          keyheight = 1, 
          keywidth = 1,
          override.aes = list(fill = legend)
          ))
      
      print(p)
      
      return(p)
    }
    
    # save full plot with only large highways
    png('img/map_bldgs_hghws.png', width = 3508, height = 3508, res=300)
    plot_loc(h=F)
    dev.off()
    
    # save plot for Pruszków
    png('img/map_bldgs_hghws_pruszkow.png', width = 3508, height = 2000, res=300)
    plot_loc('Pruszków')
    dev.off()
  }
  
  # PLOT 3 - MAP WITH METRICS
  # objects: panel_metrics
  {
      # load data
    panel_metrics <- read.xlsx('data/xlsx/panel_metrics.xlsx')

      # transform data
    locs_sp <- as(locs$osm_multipolygon, 'Spatial')
    panel_metrics_tmp <- merge(panel_metrics, locs_sp@data[,c('borough', 'osm_id')], by='borough')
    locs_sp <- tidy(locs_sp)
    locs_sp <- merge(locs_sp, panel_metrics_tmp, by.x='id', by.y='osm_id')
    
      # plot function
    plot_metrics <- function (var, title, caption, legend, breaks, labels) {
      var <- enquo(var)
      ggplot() +
        geom_polygon(data = locs_sp, aes(x=long, y=lat, group=group, fill=log(ifelse(!!var==0, 0.5, !!var))), colour='black', size=1, alpha=0.9) +
        theme_bw() +
        theme(
          plot.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank()
        ) +
        coord_fixed() +
        #scale_fill_gradientn(colors = brewer.pal(9, name = 'Greys'), name = legend, breaks = breaks, labels = labels) +
        scale_fill_viridis(breaks = breaks, labels = labels) +
        theme_void() +
        labs(
          caption = caption,
          title = title,
          fill = legend
        ) +
        theme(
          plot.title = element_text(color="#000000", face="bold", size=20, hjust=0),
          plot.caption = element_text(color="#000000", face="bold", size=12, hjust=1),
          plot.margin = unit(c(0.5,0.5,0.5,0.5), "cm"),
          legend.title = element_text(color="#505050", face="bold", size=16, hjust=1),
          legend.text = element_text(color="#505050", size=14),
          legend.position = 'bottom'
        ) +
        guides(
          fill = guide_colourbar(
            ticks.colour = "white", 
            ticks.linewidth = 1.2, 
            barwidth=15, 
            label.position="bottom"))
    }
    
    
    # plots
    a <- plot_metrics(check_in_2019, 'a) log (migrants)', 'Data Source: Polish Statistical Office', 'log # of migrants', seq(0, 6, 1),  c('0', '', '2', '', '4', '', '6'))
    b <- plot_metrics(pop_density, 'b) log (population density)', 'Data Source: Polish Statistical Office', '', seq(-1, 3, 0.5),  c('-1', '', '0', '', '1', '', '2', '', '3')) +
      labs(fill = expression(bold(paste('log # of people / ', km^2))))
    c <- plot_metrics(dist, 'c) log (distance)', 'Data Source: Google Maps', 'log km', seq(2.5, 4, 0.25), c('2.5', '', '3', '', '3.5', '', '4'))
    d <- plot_metrics(income, 'd) log (income per capita)', 'Data Source: Polish Statistical Office', 'log ratio', seq(-1.25, 0.75, 0.25), c('', '-1', '', '-0.5', '', '0', '', '0.5', ''))
    
    # save as one plot
    png('img/map_metrics_combined.png', width = 3508, height = 2480, res=300)
    plot_grid(a, b, c, d, align = 'hv', greedy=F)
    dev.off()
  }
  
  # PLOT 4 - TABLE WITH STATISTICS
  # objects: data_model
  {
    
    # rename columns to match 'official' names
    colnames(panel_metrics)[4] <- 'check_in'
    colnames(panel_metrics)[17] <- 'pop_dens'
    colnames(panel_metrics)[21] <- 'pis'
    colnames(panel_metrics)[22] <- 'ko'
    colnames(panel_metrics)[33] <- 'md'
    colnames(panel_metrics)[34] <- 'shops'
    colnames(panel_metrics)[35] <- 'tourist'
    colnames(panel_metrics)[36] <- 'leisure'
    colnames(panel_metrics)[37] <- 'sport'
    colnames(panel_metrics)[40] <- 'worship'
    colnames(panel_metrics)[41] <- 'restaurant'
    colnames(panel_metrics)[44] <- 'parcel_mean'
    
    # extract final columns and reorder
    data_model_cols <- c('check_in', 'pop_dens', 'dist', 'income', 'unempl', 'pis', 'ko', 'bu', 'bur', 'br', 'area', 'forest', 'greenery', 'kinder', 'nursery',
                         'shops', 'tourist', 'leisure', 'sport', 'restaurant', 'worship', 'bldgs_dist_mean', 'bldgs_hghws_dist_mean', 'md', 'train', 
                         'dist_waw_drive', 'time_waw_drive', 'min_dur', 'price_m2', 'parcel_n', 'parcel_mean')
    
    data_model <- panel_metrics[, data_model_cols]
    
    table_stats <- data_model %>%
      melt() %>%
      mutate(
        variable = fct_rev(variable),
        value = ifelse(variable == 'check_in' & value == 0.5, 0, value)
      ) %>%
      group_by(variable) %>%
      summarise(
        Mean = round(mean(value), 2),
        'St. Dev.' = round(sd(value), 2),
        Min = round(min(value), 2),
        Max = round(max(value), 2)
      ) %>%
      as.data.frame() %>%
      mutate(
        variable = rev(variable)
      ) %>%
      melt(variable.name = 'stat')  %>%
      ggplot (., aes(x = stat, y = rev(variable))) +
      geom_tile(fill = NA, size = 0.6, color='grey') +
      geom_text(aes(label = formatC(value, format = 'f', digits=2)), hjust=1, size = 4, color = '#505050', fontface = 'bold', nudge_x = 0.09) +
      scale_x_discrete(position = "top") +
      theme_classic() +
      theme(
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        axis.text.x = element_text(color = 'black', size = 12, face = 'bold'),
        axis.text.y = element_text(color = 'black', size = 12, face = 'bold.italic')
        )
    
    png('img/table_stats.png', width = 3508, height = 2000, res=300)
    table_stats
    dev.off()
  }
  
  # PLOT 5 - CORRELATION PLOT
  {
    png('img/corrplot.png', width = 3508, height = 3200, res=300)
    
    corrplot.mixed(
      corr = cor(data_model),
      upper = 'color',
      lower = 'number',
      upper.col = viridis(n=20),
      lower.col = 'black',
      number.cex = 0.6,
      tl.cex=0.9, 
      tl.pos='lt',
      font=3,
      tl.col = "black"
    )
    
    dev.off()
  }
  
  # PLOT 6, 7, 8 - PCA
  {
      # run pca
      pca <- principal(data_model[,-c(1, 10)], rotate="varimax", nfactors=16)
    
      # scree plot to show variance explained by each component
      pca_scree <- t(pca$Vaccounted) %>%
        as.data.frame() %>%
        rownames_to_column(.,var='Component') %>%
        arrange(desc(`Proportion Var`)) %>%
        mutate(
          Component = factor(Component, Component)
          ) %>%
        ggplot(., aes(x=Component, y=`Proportion Var`)) + 
        geom_line(group=1, color='#73D055', size=1) +
        scale_color_viridis(discrete = T) +
        geom_point(color='#73D055', size=3) + 
        geom_text(aes(y=`Proportion Var` + 0.003, label = round(`Proportion Var`, 2)), hjust=0, size = 4, fontface = 'bold', color = '#505050', nudge_x = 0.01) +
        theme_classic() +
        theme(
          axis.text.x = element_text(angle=45, vjust = 1, hjust=1, size=12),
          axis.text.y = element_text(size=12),
          axis.title = element_text(size=12, face='bold'),
          legend.title = element_text(size=12, face='bold'),
          legend.text = element_text(size=12)
        ) + 
        geom_text(aes(x = 'RC16', y = 0.155, label = paste('Cumulative Variance Explained:', round(sum(`Proportion Var`), 2))), color = '#505050', size=5, hjust=1, fontface='bold')
      
      png('img/pca_scree.png', width = 3508, height = 2480, res=300) 
      pca_scree
      dev.off()
      
      # correlation plot for the components
      png('img/pca_corrplot.png', width = 3508, height = 3508, res=300)
      
      pca_corrplot <- corrplot.mixed(
        corr = cor(pca$scores),
        upper = 'color',
        lower = 'number',
        upper.col = viridis(n=20),
        lower.col = 'black',
        number.cex = 0.7,
        tl.cex=0.9, 
        tl.pos='lt',
        font=3,
        tl.col = "black"
      )
      
      dev.off()
      
      # sort results to identify correalted groups of variables
      pca <- fa.sort(pca)
      
      # heat map to show correalted groups of variables
      pca_loadings <- data.frame(pca$Structure[pca$order,]) %>% 
        rownames_to_column("Variable") %>%
        mutate(
          Variable = factor(Variable, levels = rev(Variable))
        ) %>%
        melt(variable.name = 'Component') %>%
        mutate(
          fill = ifelse(value >= 0.5 | value <= -0.5, 'True', 'False')
        ) %>%
        ggplot(., aes(x = Component, y = Variable)) +
        scale_x_discrete(position = "top") +
        geom_tile(aes(fill=fill), size = 0.6, color='grey') +
        scale_fill_manual(name = 'Is absolute loading higher than 0.5?', values = c('True' = '#73D055', 'False' = NA)) +
        geom_text(aes(label = formatC(round(value, 2), format='f', digits=2), color = fill), 
                  size = 4, fontface='bold',
                  hjust = 1, nudge_x = 0.22) +
        scale_color_manual(name = 'Is absolute loading higher than 0.5?', values = c('True' = 'white', 'False' = 'lightgrey')) +
        theme_classic() +
        theme(
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_text(color = 'black', size = 12, face = 'bold'),
          axis.text.y = element_text(color = 'black', size = 12, face = 'bold.italic'),
          legend.position = 'top',
          legend.title = element_text(size = 12,  color = '#505050', face='bold'),
          legend.text = element_text(size = 11, color = '#505050', face='bold')
        )
      
      png('img/pca_loadings.png', width = 3508, height = 2000, res=300)
      pca_loadings
      dev.off()
    }

  # PLOT 9 - MODEL STATS TABLE
  # objects: model_stats 
  {
    # load and trransform data (melt and add identifier of best model per sample)
    model_stats <- read.xlsx('data/xlsx/model_stats.xlsx') %>%
      melt() %>%
      mutate(
        model = factor(toupper(model), levels = rev(toupper(c('ols', 'ridge', 'lasso', 'svr', 'rf', 'xgb')))),
        sample = factor(ifelse(variable %in% c('rmse_valid', 'mae_valid', 'r2_valid'), 'Validation Sample', 'Training Sample'), 
                        levels = c('Validation Sample', 'Training Sample')),
        metric = factor(ifelse(variable %in% c('rmse_valid', 'rmse_train'), 'RMSE',
                               ifelse(variable %in% c('mae_valid', 'mae_train'), 'MAE', 'R2')),
                        levels = c('RMSE', 'MAE', 'R2')),
        best_valid = ifelse(model == model[value == max(value[sample == 'Validation Sample' & metric == 'R2'])] & sample == 'Validation Sample', T, F),
        best_train = ifelse(model == model[value == max(value[sample == 'Training Sample' & metric == 'R2'])] & sample == 'Training Sample', T, F),
        best = ifelse(best_valid | best_train, 'T', 'F')
      ) %>%
      dplyr::select (model, sample, metric, value, best)
    
    # plot
    table_model_stats <- ggplot(model_stats, aes(x = metric, y = model)) +
      geom_tile(aes(fill = best), size = 0.6, color='grey') +
      scale_fill_manual(values = c('T' = '#73D055', 'F' = NA)) +
      geom_text(aes(label = formatC(value, format = 'f', digits=2), color = best), 
                hjust=1, size = 4, fontface = 'bold', nudge_x = 0.09) +
      scale_color_manual(values = c('T' = 'white', 'F' = '#505050')) +
      facet_grid(cols = vars(sample), scales = "free", space = "free") +
      scale_x_discrete(position = "top") +
      theme_classic() +
      theme(
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        axis.text.x = element_text(color = 'black', size = 11, face = 'bold'),
        axis.text.y = element_text(color = 'black', size = 12, face = 'bold'),
        strip.placement = 'outside',
        strip.text = element_text(color = 'black', size = 12, face = 'bold'),
        strip.background = element_blank(),
        legend.position = 'none'
      )
    
    # save plot
    png('img/table_model_stats.png', width = 3508, height = 1500, res=300) 
    table_model_stats
    dev.off()
  }
  
  # PLOT 10 & 11
  {
    # load data
    data_model_final <- panel_metrics[, c(4, 6, 10:17, 19:20, 22:23, 28, 34:36, 40, 42:44)]
    y <- as.matrix(data_model_final[,1]) # for ridge & lasso
    x <- as.matrix(data_model_final[,-1]) # for ridge & lasso
    data_model_final_dmatrix <- xgb.DMatrix(data = data.matrix(data_model_final[,-1]), label = data_model_final[,1]) # for xgb
    
    # calculate models
      # ridge
      ridge  <- glmnet(x, y, alpha=0, lambda = 278.8641)
      # lasso
      lasso  <- glmnet(x, y, alpha=1, lambda = 9.566347)
      # ols
      ols <- lm(check_in ~ dist + nursery + pop_dens + train, data = data_model_final)
      # svr
      svr <- svm(check_in~., data=data_model_final, kernel='radial', epsilon = 0.13, cost = 207, gamma = 0.002)
      # rf
      set.seed(435632)
      rf <- randomForest(check_in~., data=data_model_final, importance = T, ntree=17, mtry=10, replace=F, nodesize=12, maxnode=7)
      # xgb
      xgb_params <- list(
        booster = 'gbtree',
        eta = 0.36,
        gamma = 0,
        alpha = 40,
        lambda = 0,
        max_depth = 3,
        min_child_weight = 7,
        subsample = 0.8,
        colsample_bytree = 0.4,
        tree_method = 'exact',
        objective = 'reg:squarederror',
        eval_metric = 'rmse'
      )
      set.seed(435632)
      xgb_gbtree <- xgb.train(params = xgb_params, data = data_model_final_dmatrix, nrounds = 8, verbose = 2)
    
    # add prediction functions
      # ridge
      yhat_ridge <- function(X.model, newdata) as.numeric(predict(X.model, s=278.8641, newx=newdata))
      # lasso
      yhat_lasso <- function(X.model, newdata) as.numeric(predict(X.model, s=9.566347, newx=newdata))
      # ols, svr, rf
      yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata))
      # xgb
      yhat_xgb <- function(X.model, newdata) as.numeric(predict(X.model, data.matrix(newdata)))
      
    # PFI calculation - long computation time
    {
      # # RIDGE
      # set.seed(435632)
      # vi_ridge <- as.data.frame(vi_permute(ridge, train = as.matrix(data_model_final[,-1]), target = as.matrix(data_model_final[,1]), metric='rmse', pred_wrapper = yhat_ridge, type='ratio', nsim=1000))
      # vi_ridge <- vi_ridge %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_ridge = row_number(),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev)
      #   ) %>%
      #   rename(
      #     mean_imp_ridge = Importance,
      #     sd_imp_ridge = StDev)
      # 
      # # LASSO
      # set.seed(435632)
      # vi_lasso <- as.data.frame(vi_permute(lasso, train = as.matrix(data_model_final[,-1]), target = as.matrix(data_model_final[,1]), metric='rmse', pred_wrapper = yhat_lasso, type='ratio', nsim=1000))
      # vi_lasso <- vi_lasso %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_lasso = ifelse(Importance==1, NA, row_number()),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev)
      #   ) %>%
      #   rename(
      #     mean_imp_lasso = Importance,
      #     sd_imp_lasso = StDev)
      # 
      # # OLS
      # set.seed(435632)
      # vi_ols <- as.data.frame(vi_permute(ols, train = data_model_final, target = 'check_in', metric='rmse', pred_wrapper = yhat,  type='ratio', nsim=1000))
      # vi_ols <- vi_ols %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_ols = ifelse(Importance==1, NA, row_number()),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev)
      #   ) %>%
      #   rename(
      #     mean_imp_ols = Importance,
      #     sd_imp_ols = StDev
      #   )
      # 
      # # SVR
      # set.seed(435632)
      # vi_svr <- as.data.frame(vi_permute(svr, train = data_model_final, target = 'check_in', metric='rmse', pred_wrapper = yhat,  type='ratio', nsim=1000))
      # vi_svr <- vi_svr %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_svr = row_number(),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev)
      #   ) %>%
      #   rename(
      #     mean_imp_svr = Importance,
      #     sd_imp_svr = StDev
      #   )
      # 
      # # RF
      # set.seed(435632)
      # vi_rf <- as.data.frame(vi_permute(rf, train = data_model_final, target = 'check_in', metric='rmse', pred_wrapper = yhat,  type='ratio', nsim=1000))
      # vi_rf <- vi_rf %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_rf = ifelse(Importance==1, NA, row_number()),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev) 
      #   ) %>%
      #   rename(
      #     mean_imp_rf = Importance,
      #     sd_imp_rf = StDev
      #   )
      # 
      # # XGB
      # set.seed(435632)
      # vi_xgb <- as.data.frame(vi_permute(xgb_gbtree, train = data.matrix(data_model_final[,-1]), target = data_model_final[,1], metric='rmse', pred_wrapper = yhat_xgb,  type='ratio', nsim=1000))
      # vi_xgb <- vi_xgb %>% 
      #   arrange(desc(ifelse(Importance==1, NA, Importance))) %>%
      #   mutate(
      #     rank_xgb = ifelse(Importance==1, NA, row_number()),
      #     Importance = ifelse(Importance==1, NA,  Importance - 1),
      #     StDev = ifelse(StDev==0, NA,  StDev)
      #   ) %>%
      #   rename(
      #     mean_imp_xgb = Importance,
      #     sd_imp_xgb = StDev
      #   )
      # 
      # # consolidated df
      # vi <- merge(merge(merge(merge(merge(vi_ridge, vi_lasso, by='Variable'), vi_ols, by='Variable'), vi_svr, by='Variable'), vi_rf), vi_xgb, by='Variable')
      # 
      # vi_rank <- vi %>%
      #   rowwise() %>%
      #   mutate(mean_imp_mean = mean(c(mean_imp_ridge, mean_imp_lasso, mean_imp_ols, mean_imp_svr, mean_imp_rf, mean_imp_xgb), na.rm=T),
      #          sd_imp_mean = mean(c(sd_imp_ridge, sd_imp_lasso, sd_imp_ols, sd_imp_svr, sd_imp_rf, sd_imp_xgb), na.rm=T),
      #          rank_mean = mean(c(rank_ridge, rank_lasso, rank_ols, rank_svr, rank_rf, rank_xgb), na.rm=T)
      #   ) %>%
      #   arrange(desc(mean_imp_mean)) %>%
      #   as.data.frame()
        
      #  # save PFI file
      #  write.xlsx(vi_rank, 'data/xlsx/Var_Rank.xlsx')
      }
      
    # load saved file    
    var_rank <- read.xlsx('data/xlsx/Var_Rank.xlsx') %>%
      arrange(desc(mean_imp_mean)) %>%
      dplyr::select(Variable, mean_imp_ridge, mean_imp_lasso, mean_imp_ols, mean_imp_svr, mean_imp_rf, mean_imp_xgb, mean_imp_mean) %>%
      melt(variable.name = 'Model') %>%
      #filter(!is.na(value)) %>%
      arrange(Model, desc(value)) %>%
      group_by(Model) %>%
      filter(
        row_number() <=6
      ) %>%
      ungroup() %>%
      dplyr::mutate(
        Variable = ifelse(is.na(value), '', Variable),
        Model = recode(
          Model,
          'mean_imp_ridge' = 'RIDGE',
          'mean_imp_lasso' = 'LASSO',
          'mean_imp_ols' = 'OLS',
          'mean_imp_svr' = 'SVR',
          'mean_imp_rf' = 'RF',
          'mean_imp_xgb' = 'XGB',
          'mean_imp_mean' = 'MEAN OF ALL MODELS'
          ),
        Rank = factor(row_number(), levels = (42:1))
        )
    
    # first plot  
    var_rank_models <- ggplot(var_rank [var_rank$Model != 'MEAN OF ALL MODELS',], aes(y = Rank, x = value, label=round(value, 2))) +
      geom_bar(stat='identity', color = viridis(1), fill = viridis(1), width = 0.8) +
      geom_text(aes(x=value + 0.03), size = 3, color = '#505050', hjust=0, fontface='bold') +
      facet_wrap(~Model, scales = 'free_y', ncol=2) +
      scale_x_continuous(limits = c(0, 1.25)) + 
      scale_y_discrete(
        breaks = var_rank$Rank,
        labels = var_rank$Variable,
        expand = c(0.2,0.2)
      ) +
      theme_classic() +
      theme(
        axis.line=element_blank(),
        axis.title=element_blank(),
        axis.ticks=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y = element_text(hjust=1, face='bold', color='black', size=9),
        strip.text = element_text(color = 'white', size = 10, face = 'bold'),
        strip.background = element_rect(fill = '#505050', color = '#505050')
      )
  
    # second plot
    var_rank_mean <- ggplot(var_rank [var_rank$Model == 'MEAN OF ALL MODELS',], aes(y = reorder(Variable, value), x = value, label=round(value, 2))) +
      geom_bar(stat='identity', color = viridis(1), fill = viridis(1), width = 0.8) +
      geom_text(aes(x=value + 0.02), size = 4, color = '#505050', hjust=0, fontface='bold') +
      facet_wrap(~Model, scales = 'free_y') +
      scale_x_continuous(limits = c(0, 1.25)) + 
      theme_classic() +
      theme(
        axis.line=element_blank(),
        axis.title=element_blank(),
        axis.ticks=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y = element_text(hjust=1, face='bold', color='black', size=13),
        strip.text = element_text(color = 'white', size = 14, face = 'bold'),
        strip.background = element_rect(fill = '#505050', color = '#505050')
      )
    
    # combine plots and save
    png('img/var_rank.png', width = 3508, height = 2000, res=300) 
    grid.arrange(
      var_rank_models,
      var_rank_mean,
      ncol=2
    )
    dev.off()
    
    # fix scales
    scale_color <- viridis(6)
    names(scale_color) <- c('ridge', 'lasso', 'ols', 'svr', 'rf', 'xgb')
    scale_line <- c('solid', 'twodash', 'longdash', 'dotted', 'dotdash', 'dashed')
    names(scale_line) <- c('ridge', 'lasso', 'ols', 'svr', 'rf', 'xgb')
    
    # function
    ALEplot <- function (var_nm, title, xtitle) {

      var <- which(colnames(data_model_final) == var_nm) - 1
      
      ale_ridge <- as.data.frame(ALEPlot(as.matrix(data_model_final[,-1]), ridge, pred.fun=yhat_ridge, J=var, K=70, NA.plot = TRUE))
      ale_ridge$model <- 'ridge'
      ale_lasso <- as.data.frame(ALEPlot(as.matrix(data_model_final[,-1]), lasso, pred.fun=yhat_lasso, J=var, K=70, NA.plot = TRUE))
      ale_lasso$model <- 'lasso'
      ale_ols <- as.data.frame(ALEPlot(data_model_final[,-1], ols, pred.fun=yhat, J=var, K=70, NA.plot = TRUE))
      ale_ols$model <- 'ols'
      ale_svr <- as.data.frame(ALEPlot(data_model_final[,-1], svr, pred.fun=yhat, J=var, K=70, NA.plot = TRUE))
      ale_svr$model <- 'svr'
      ale_rf <- as.data.frame(ALEPlot(data_model_final[,-1], rf, pred.fun=yhat, J=var, K=70, NA.plot = TRUE))
      ale_rf$model <- 'rf'
      ale_xgb <- as.data.frame(ALEPlot(data_model_final[,-1], xgb_gbtree, pred.fun=yhat_xgb, J=var, K=70, NA.plot = TRUE))
      ale_xgb$model <- 'xgb'
      
      ale <- rbind(rbind(rbind(rbind(rbind(ale_ridge, ale_lasso), ale_ols), ale_svr), ale_rf), ale_xgb)
      
      ale_drop <- ale %>%
        group_by(model) %>%
        dplyr::summarise(sum = sum(f.values)) %>%
        as.data.frame() %>%
        filter(sum==0)
      
      ale_drop <- c(ale_drop$model)
      
      ale <- ale %>%
        filter(!model %in% ale_drop)
      
      plt <- ggplot(ale, aes(x= x.values, y = f.values, linetype = model, color = model)) +
        geom_line(size=1) +
        scale_colour_manual(name = 'model', values = scale_color) +
        scale_linetype_manual(name = 'model', values = scale_line) +
        labs(
          #title = title,
          y = 'number of migrants',
          x = xtitle
        ) +
        theme_classic() + 
        theme(
          axis.text = element_text(size=14),
          panel.grid.major.y = element_line(size=0.4, color = 'lightgrey'),
          panel.grid.minor.y = element_line(size=0.2, color = 'lightgrey'),
          axis.title = element_text(size=14, face = 'bold'),
          legend.title = element_text(size=14, face='bold'),
          legend.text = element_text(size=14),
          panel.border = element_rect(colour = "black", fill=NA, size=1.2)
        )
      
      print(plt)
      
      return(plt)
    }
    
    # create plots for top 6 variables by PFI
    ALE1 <- ALEplot('nursery', 'a) numer of places in nurseries', 'number of places available')
    ALE2 <- ALEplot('dist', 'b) distance (straight line)', 'kilometers')
    ALE3 <- ALEplot('income', 'c) relative income per capita', 'mean income per capita relative to the mean in all boroughs')
    ALE4 <- ALEplot('ko', 'd) percentage of votes for KO in 2019', 'percentage') +
      scale_x_continuous(labels = scales::percent)
    ALE5 <- ALEplot('worship', 'e) number of places of worship', 'number of sites')
    ALE6 <- ALEplot('pop_dens', 'f) population density', '') +
      labs(x = expression(bold(paste('number of people per ', km^2))))
    
    # saved consolidated plot
    png('img/ALE.png', width = 3508, height = 2480, res=300)
    grid.arrange(
      ALE1,
      ALE2,
      ALE3,
      ALE4,
      ALE5,
      ALE6,
      ncol=2, nrow=3)
    dev.off()
  }
  
png('img/ALE_dist.png', width = 3508, height = 2480, res=300)
ALE2
dev.off()
      
  